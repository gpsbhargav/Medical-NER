{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from nltk import conlltags2tree\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import tree2conlltags\n",
    "from nltk.chunk import ChunkParserI\n",
    "from nltk import pos_tag,pos_tag_sents \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from pprint import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import dill as pickle\n",
    "import os\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK resource downloads:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_GLOVE = True\n",
    "GLOVE_DIR = '/home/gpsbhargav/NLU/assignment3/project/resources/'\n",
    "GLOVE_NAME = 'glove.6B.50d.float.pkl'\n",
    "GLOVE_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickler(path,pkl_name,obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def unpickler(path,pkl_name):\n",
    "    with open(os.path.join(path, pkl_name) ,'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_GLOVE:\n",
    "    glove = unpickler(GLOVE_DIR,GLOVE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, features=None,label=None):\n",
    "        if(features == None):\n",
    "            self.features = []\n",
    "        else:\n",
    "            self.features = features\n",
    "        self.label = label\n",
    "    \n",
    "    def add_feature(self,feature):\n",
    "        self.features.append(feature)\n",
    "    \n",
    "    def add_features(self,features):\n",
    "        for feature in features:\n",
    "            self.features.append(feature)\n",
    "    \n",
    "    def get_string(self,only_word=False,label=False,new_line=False):\n",
    "        if(only_word):\n",
    "            return self.features[0]\n",
    "        s = \" \".join(map(str, self.features))\n",
    "        if(label):\n",
    "            s = s + \" \" + self.label\n",
    "        if(new_line):\n",
    "            s = s + \"\\n\"\n",
    "        return s\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self,tokens=None):\n",
    "        if(tokens == None):\n",
    "            self.tokens = []\n",
    "        else:\n",
    "            self.tokens = tokens\n",
    "\n",
    "    def add_token(self,token):\n",
    "        #print(\"Before appending: \",self.get_in_format())\n",
    "        self.tokens.append(token)\n",
    "\n",
    "    def get_num_tokens(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def get_token_list(self):\n",
    "        tok_list = []\n",
    "        for token in self.tokens:\n",
    "            tok_list.append(token.features[0])\n",
    "        return tok_list\n",
    "    \n",
    "    def get_in_format(self,only_word=False,label=False,new_line=False):\n",
    "        sent_list = []\n",
    "        for token in self.tokens:\n",
    "            token_string = token.get_string(only_word=False,label=label,new_line=new_line)\n",
    "            sent_list.append(token_string)\n",
    "        if(new_line):\n",
    "            sent_list.append('\\n')\n",
    "        return sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path,label=True):\n",
    "    data = []\n",
    "    count = 0\n",
    "    lines = []\n",
    "    sentence = Sentence()\n",
    "    with open(path,'r',encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            if(line == '\\n'):\n",
    "                count += 1\n",
    "                #if(sentence.get_num_tokens() > 0):\n",
    "                data.append(sentence)\n",
    "                sentence = Sentence()\n",
    "                #print(\"new sentence created: \",sentence)\n",
    "                #print(sentence.get_in_format())\n",
    "            else:\n",
    "                token = []\n",
    "                contents = line.split()\n",
    "                #print(contents)\n",
    "                if(label):\n",
    "                    token = Token(contents[:-1],contents[-1])\n",
    "                else:\n",
    "                    token = Token(contents)\n",
    "\n",
    "                #print(token.features)\n",
    "                #print(token.label)\n",
    "                #print(token.get_string(True))\n",
    "                sentence.add_token(token)\n",
    "                #print(\"sentence contents: \",sentence.get_in_format())\n",
    "            \n",
    "    print(\"Number of sentences: \",count)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(data,path,label=True,new_line=True):\n",
    "    with open(path,'w+',encoding='latin1') as f:\n",
    "        for sentence in tqdm(data):\n",
    "            f.writelines(sentence.get_in_format(label=label,new_line=new_line))\n",
    "            #print(sentence.get_in_format(label))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  2924\n",
      "Number of sentences:  731\n"
     ]
    }
   ],
   "source": [
    "original_data = read_file(\"../train.txt\")\n",
    "test_data_original = read_file(\"../test.txt\",label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(word):\n",
    "    word_shape = 'other'\n",
    "    if re.match('[0-9]+(\\.[0-9]*)?|[0-9]*\\.[0-9]+$', word):\n",
    "        word_shape = 'number'\n",
    "    elif re.match('\\W+$', word):\n",
    "        word_shape = 'punct'\n",
    "    elif re.match('[A-Z][a-z]+$', word):\n",
    "        word_shape = 'capitalized'\n",
    "    elif re.match('[A-Z]+$', word):\n",
    "        word_shape = 'uppercase'\n",
    "    elif re.match('[a-z]+$', word):\n",
    "        word_shape = 'lowercase'\n",
    "    elif re.match('[A-Z][a-z]+[A-Z][a-z]+[A-Za-z]*$', word):\n",
    "        word_shape = 'camelcase'\n",
    "    elif re.match('[A-Za-z]+$', word):\n",
    "        word_shape = 'mixedcase'\n",
    "    elif re.match('__.+__$', word):\n",
    "        word_shape = 'wildcard'\n",
    "    elif re.match('[A-Za-z0-9]+\\.$', word):\n",
    "        word_shape = 'ending-dot'\n",
    "    elif re.match('[A-Za-z0-9]+\\.[A-Za-z0-9\\.]+\\.$', word):\n",
    "        word_shape = 'abbreviation'\n",
    "    elif re.match('[A-Za-z0-9]+\\-[A-Za-z0-9\\-]+.*$', word):\n",
    "        word_shape = 'contains-hyphen'\n",
    " \n",
    "    return word_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_path = \"../resources/\"\n",
    "mesh_cat_pkl = \"categorized_terms_ABCDEJN_BOW.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_mesh_terms = unpickler(resource_path,mesh_cat_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mesh_categories(word):\n",
    "    word = word.lower()\n",
    "    tags = []\n",
    "    for cat in categorized_mesh_terms.keys():\n",
    "        if word in categorized_mesh_terms[cat]:\n",
    "            tags.append(\"y_\"+cat)\n",
    "        else:\n",
    "            tags.append(\"n_\"+cat)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n_D', 'n_J', 'n_A', 'y_C', 'n_B', 'n_N', 'n_E']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mesh_categories(\"sarcoidosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prefixes_and_suffixes(word):\n",
    "    word = word.lower()\n",
    "    return [word[:1],word[:2],word[:3],word[:4],word[-1:],word[-2:],word[-3:],word[-4:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "lmtzr = WordNetLemmatizer() \n",
    " \n",
    "def ner_features(tokens, index, history=None):\n",
    "    \"\"\"\n",
    "    tokens  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    index   = the index of the token we want to extract features for\n",
    "    history = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    "\n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('__START2__', '__START2__'), ('__START1__', '__START1__')] + list(tokens) + [('__END1__', '__END1__'), ('__END2__', '__END2__')]\n",
    "    #history = ['__START2__', '__START1__'] + list(history)\n",
    "    useful_pos = ['NN','JJ','NNS','NNP','CC','IN'\n",
    "                 #for O:\n",
    "                 #'IN','DT','CC','CD','VBN','VBD'\n",
    "                 ]\n",
    "\n",
    "\n",
    "    # shift the index with 2, to accommodate the padding\n",
    "    index += 2\n",
    "\n",
    "    word, pos = tokens[index]\n",
    "#     pos = pos if pos in useful_pos else 'Other'\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    #previob = history[-1]\n",
    "    #prevpreviob = history[-2]\n",
    "    \n",
    "    mesh_categories = get_mesh_categories(word)\n",
    "    \n",
    "    \n",
    "    features= [\n",
    "        pos,\n",
    "#         shape(word),\n",
    "        lmtzr.lemmatize(word),\n",
    "\n",
    "         nextword,\n",
    "         nextpos,\n",
    "         lmtzr.lemmatize(nextword),\n",
    "#          shape(nextword),\n",
    "\n",
    "#         nextnextword,\n",
    "#         nextnextpos,\n",
    "#         lmtzr.lemmatize(nextnextword),\n",
    "#         shape(nextnextword),\n",
    "\n",
    "        prevword,\n",
    "        prevpos,\n",
    "        lmtzr.lemmatize(prevword),\n",
    "#         shape(prevword),\n",
    "\n",
    "#         prevprevword,\n",
    "#         prevprevpos,\n",
    "#         lmtzr.lemmatize(prevprevword),\n",
    "#         shape(prevprevword)\n",
    "    ]\n",
    "    \n",
    "#    features = features + mesh_categories\n",
    "#    features = mesh_categories\n",
    "    \n",
    "    if(LOAD_GLOVE):\n",
    "       if(word.lower() in glove.keys()):\n",
    "           g_features = glove[word.lower()].tolist()\n",
    "       else:\n",
    "           g_features = [0] * GLOVE_SIZE\n",
    "    \n",
    "    features = features + get_prefixes_and_suffixes(word) + mesh_categories + g_features\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "feat_dict = {\n",
    "        #'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "        'pos': pos,\n",
    "        'shape': shape(word),\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-pos': nextpos,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-shape': shape(nextword),\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'next-next-pos': nextnextpos,\n",
    "        'next-next-lemma': stemmer.stem(nextnextword),\n",
    "        'next-next-shape': shape(nextnextword),\n",
    " \n",
    "        'prev-word': prevword,\n",
    "        'prev-pos': prevpos,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        #'prev-iob': previob,\n",
    "        'prev-shape': shape(prevword),\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    "        'prev-prev-lemma': stemmer.stem(prevprevword),\n",
    "        #'prev-prev-iob': prevpreviob,\n",
    "        'prev-prev-shape': shape(prevprevword),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2924/2924 [00:07<00:00, 385.83it/s]\n"
     ]
    }
   ],
   "source": [
    "new_data = original_data.copy()\n",
    "for data in tqdm(new_data):\n",
    "    tokens_str = pos_tag(data.get_in_format(only_word=True))\n",
    "    for i,token in enumerate(data.tokens):\n",
    "        features = ner_features(tokens_str,i)\n",
    "        token.add_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 731/731 [00:01<00:00, 570.18it/s]\n"
     ]
    }
   ],
   "source": [
    "new_test_data = test_data_original.copy()\n",
    "for data in tqdm(new_test_data):\n",
    "    tokens_str = pos_tag(data.get_in_format(only_word=True))\n",
    "    for i,token in enumerate(data.tokens):\n",
    "        features = ner_features(tokens_str,i)\n",
    "        token.add_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Does NNP Does blinding NN blinding __START1__ __START1__ __START1__ d do doe does s es oes does n_D n_J n_A n_C n_B n_N n_E 0.22930000722408295 0.34231001138687134 0.05981700122356415 0.08300299942493439 0.5768499970436096 0.28852999210357666 -0.011265999637544155 -0.1784600019454956 0.16947999596595764 0.327349990606308 0.14047999680042267 0.8287000060081482 -0.2763200104236603 -0.14559000730514526 0.8733000159263611 1.0743999481201172 0.5294100046157837 0.006656699813902378 0.414249986410141 -0.7607600092887878 -0.4442799985408783 0.03714999929070473 0.01876699924468994 0.3484399914741516 0.48342999815940857 -2.188199996948242 -0.7182199954986572 0.11967000365257263 0.7330099940299988 -0.7412199974060059 3.1152000427246094 0.2626500129699707 -0.8239200115203857 -0.4830799996852875 -0.26423001289367676 -0.4987199902534485 0.13840000331401825 -0.1821800023317337 0.24401000142097473 -0.5337700247764587 0.05577300116419792 0.36048001050949097 -0.10412999987602234 0.3241199851036072 -0.11661999672651291 0.22120000422000885 0.19864000380039215 0.191430002450943 -0.04152800142765045 1.0860999822616577\\n',\n",
       " 'blinding NN blinding of IN of Does NNP Does b bl bli blin g ng ing ding n_D n_J n_A n_C n_B n_N n_E -0.1898300051689148 0.14583000540733337 0.6534799933433533 0.32910001277923584 -1.1820000410079956 -0.06729699671268463 1.0196000337600708 0.11180000007152557 0.8713899850845337 1.1160000562667847 0.06599999964237213 -0.20619000494480133 0.5476599931716919 0.07621099799871445 -0.2523300051689148 0.2828400135040283 -1.108299970626831 0.05557499825954437 -0.9607599973678589 -0.8386399745941162 -0.7395600080490112 0.23074999451637268 0.4909699857234955 -1.3738000392913818 0.9624900221824646 -0.46349000930786133 -0.22454999387264252 1.1469999551773071 1.833299994468689 0.18619999289512634 -0.09478200227022171 0.6184800267219543 -0.23326000571250916 -0.9414399862289429 -0.4345700144767761 0.6102700233459473 0.23850999772548676 -0.10656999796628952 -0.43276000022888184 0.11670000106096268 -0.22224999964237213 0.8706700205802917 -0.2573600113391876 -0.14607000350952148 0.6073499917984009 0.09368900209665298 0.4906400144100189 -0.9524400234222412 0.12110000103712082 -0.6533899903297424\\n',\n",
       " 'of IN of readers NNS reader blinding NN blinding o of of of f of of of y_D y_J y_A y_C y_B y_N y_E 0.7085300087928772 0.5708799958229065 -0.4715999960899353 0.1804800033569336 0.5444899797439575 0.7260299921035767 0.18156999349594116 -0.523930013179779 0.10380999743938446 -0.17565999925136566 0.07885199785232544 -0.36215999722480774 -0.1182899996638298 -0.8333600163459778 0.11917000263929367 -0.1660500019788742 0.06155499815940857 -0.01271899975836277 -0.5662299990653992 0.013616000302135944 0.22851000726222992 -0.14395999908447266 -0.06754899770021439 -0.3815700113773346 -0.23698000609874725 -1.7036999464035034 -0.866919994354248 -0.2670400142669678 -0.258899986743927 0.17669999599456787 3.8675999641418457 -0.16130000352859497 -0.13273000717163086 -0.6888099908828735 0.1844400018453598 0.005246399901807308 -0.3387399911880493 -0.0789560005068779 0.24185000360012054 0.3657599985599518 -0.34727001190185547 0.2848300039768219 0.07569299638271332 -0.062178000807762146 -0.3898800015449524 0.22901999950408936 -0.21616999804973602 -0.22562000155448914 -0.09391800314188004 -0.8037499785423279\\n',\n",
       " 'readers NNS reader affect VBP affect of IN of r re rea read s rs ers ders n_D n_J n_A n_C n_B n_N n_E -0.14354999363422394 0.3094399869441986 0.7537299990653992 -0.8611199855804443 0.4770500063896179 -0.2479500025510788 -0.9353899955749512 -0.24761000275611877 -0.007429100107401609 0.3241199851036072 -0.6053299903869629 0.248089998960495 0.9582099914550781 -0.16110000014305115 0.8678799867630005 -0.4290100038051605 -0.5443300008773804 -0.46105000376701355 0.26857998967170715 -0.6834099888801575 0.4852199852466583 0.3511199951171875 0.7279899716377258 1.5490000247955322 1.486799955368042 -1.017199993133545 -1.3756999969482422 -0.783270001411438 -0.15140999853610992 -0.5664700269699097 2.0408999919891357 0.4668799936771393 0.2879300117492676 -0.8898800015449524 -1.0226999521255493 -0.28902000188827515 -0.6029700040817261 -0.04912000149488449 -0.6461399793624878 -1.0698000192642212 1.5663000345230103 0.03886500000953674 0.09987399727106094 0.8491399884223938 0.23593999445438385 0.2962999939918518 0.3948099911212921 0.41027000546455383 -0.04361699894070625 0.39614999294281006\\n',\n",
       " 'affect VBP affect results NNS result readers NNS reader a af aff affe t ct ect fect n_D n_J n_A n_C n_B n_N n_E 0.5184000134468079 -0.04427700117230415 -0.08368799835443497 0.5339300036430359 -0.10687000304460526 0.48278000950813293 0.31286999583244324 0.23789000511169434 0.69691002368927 0.21916000545024872 0.6332899928092957 -0.0382549986243248 0.33755001425743103 -0.21883000433444977 0.5864400267601013 0.940530002117157 -0.027557000517845154 -0.3169400095939636 0.43727999925613403 -0.714460015296936 -0.056331001222133636 -0.39135000109672546 0.16078999638557434 -0.12291000038385391 0.4147599935531616 -0.8520200252532959 -0.1848600059747696 -0.13023999333381653 0.7284799814224243 0.5276700258255005 2.88070011138916 0.976580023765564 0.2284799963235855 -1.0432000160217285 -0.19976000487804413 -0.5267000198364258 -0.3732900023460388 -0.273470014333725 -0.07157500088214874 0.03352700173854828 -1.2200000286102295 -0.19008000195026398 0.436379998922348 0.3061999976634979 0.0806640014052391 0.14303000271320343 0.0009320200188085437 0.8461899757385254 -0.05537699908018112 0.566789984703064\\n',\n",
       " 'results NNS result of IN of affect VBP affect r re res resu s ts lts ults n_D n_J n_A n_C n_B y_N y_E 0.087738998234272 0.12049999833106995 -0.05546199902892113 1.250100016593933 -0.39726001024246216 -0.2053299993276596 0.16940000653266907 0.5474399924278259 0.2221899926662445 0.1748500019311905 0.5090699791908264 -1.1684999465942383 -0.7499499917030334 -0.4990899860858917 0.975570023059845 0.6039100289344788 -0.4702700078487396 -1.0152000188827515 -0.5682299733161926 -1.1347999572753906 0.23463000357151031 -0.26927000284194946 0.8512499928474426 0.004757999908179045 0.03243099898099899 -0.8334000110626221 -1.4179999828338623 -0.09037700295448303 -0.879289984703064 0.5308600068092346 3.182800054550171 0.40264999866485596 -0.40856999158859253 -1.065500020980835 0.21220999956130981 -0.9250400066375732 0.5479999780654907 1.1507999897003174 0.036407001316547394 -0.9427599906921387 -0.20125000178813934 -0.8436200022697449 -0.11281000077724457 0.23204000294208527 0.22245000302791595 0.20521999895572662 0.30897000432014465 1.1533000469207764 0.9841899871826172 -0.17332999408245087\\n',\n",
       " 'of IN of meta-analyses NNS meta-analyses results NNS result o of of of f of of of y_D y_J y_A y_C y_B y_N y_E 0.7085300087928772 0.5708799958229065 -0.4715999960899353 0.1804800033569336 0.5444899797439575 0.7260299921035767 0.18156999349594116 -0.523930013179779 0.10380999743938446 -0.17565999925136566 0.07885199785232544 -0.36215999722480774 -0.1182899996638298 -0.8333600163459778 0.11917000263929367 -0.1660500019788742 0.06155499815940857 -0.01271899975836277 -0.5662299990653992 0.013616000302135944 0.22851000726222992 -0.14395999908447266 -0.06754899770021439 -0.3815700113773346 -0.23698000609874725 -1.7036999464035034 -0.866919994354248 -0.2670400142669678 -0.258899986743927 0.17669999599456787 3.8675999641418457 -0.16130000352859497 -0.13273000717163086 -0.6888099908828735 0.1844400018453598 0.005246399901807308 -0.3387399911880493 -0.0789560005068779 0.24185000360012054 0.3657599985599518 -0.34727001190185547 0.2848300039768219 0.07569299638271332 -0.062178000807762146 -0.3898800015449524 0.22901999950408936 -0.21616999804973602 -0.22562000155448914 -0.09391800314188004 -0.8037499785423279\\n',\n",
       " 'meta-analyses NNS meta-analyses ? . ? of IN of m me met meta s es ses yses n_D n_J n_A n_C n_B n_N n_E 1.0002000331878662 -0.4472300112247467 -0.49401000142097473 -0.7763400077819824 -1.2755999565124512 0.221110001206398 0.30678999423980713 -0.1874600052833557 0.1244800016283989 1.0506999492645264 -0.1100199967622757 -0.8802899718284607 0.12658999860286713 -0.3883399963378906 0.34029000997543335 -0.6669399738311768 -0.04543700069189072 -1.1647000312805176 -0.1310500055551529 0.1850000023841858 0.007839400321245193 0.1035899966955185 1.2455999851226807 0.2210800051689148 -0.39355000853538513 1.7890000343322754 -0.5989500284194946 -0.14443999528884888 -1.0578999519348145 0.14774000644683838 -0.5882899761199951 -0.5329300165176392 0.739549994468689 -1.3250999450683594 0.12291999906301498 -0.15222999453544617 0.3635300099849701 0.427480012178421 -0.41561999917030334 0.2882100045681 0.2524999976158142 -0.7501000165939331 0.5326399803161621 0.6509799957275391 0.35275998711586 -0.23884999752044678 0.5148599743843079 1.43149995803833 -0.5094500184059143 -0.35923001170158386\\n',\n",
       " '? . ? __END1__ __END1__ __END1__ meta-analyses NNS meta-analyses ? ? ? ? ? ? ? ? n_D n_J n_A n_C n_B n_N n_E -0.14577999711036682 0.5045899748802185 0.04752499982714653 -0.46463000774383545 0.44249001145362854 -0.1677200049161911 -0.40334001183509827 -0.39223000407218933 -0.4154300093650818 0.27636998891830444 -0.6302700042724609 0.6903300285339355 -0.45440998673439026 0.0015844999579712749 1.312000036239624 0.5241299867630005 0.37380000948905945 0.2815600037574768 -0.004056300036609173 -0.526639997959137 -0.570609986782074 0.3656100034713745 0.5917400121688843 0.3471300005912781 0.4500899910926819 -2.145400047302246 -1.3795000314712524 0.3070000112056732 1.4875999689102173 -0.963129997253418 2.8403000831604004 0.5024700164794922 -0.8675199747085571 0.06413000077009201 -0.36375999450683594 -0.1401900053024292 0.1197500005364418 -0.04544200003147125 0.7268199920654297 -0.44446998834609985 -0.27226001024246216 0.15029999613761902 0.11489000171422958 0.712369978427887 0.11341000348329544 0.22834999859333038 -0.040800999850034714 -0.41468000411987305 0.1105400025844574 1.1680999994277954\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_data[0].get_in_format(label=False,new_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sarcoidosis NN Sarcoidosis involving VBG involving __START1__ __START1__ __START1__ s sa sar sarc s is sis osis n_D n_J n_A y_C n_B n_N n_E 1.3918999433517456 0.23127999901771545 -0.16245000064373016 0.19147999584674835 -1.142300009727478 1.1601999998092651 0.6109700202941895 0.5369200110435486 -0.018831999972462654 0.5237600207328796 0.19789999723434448 -0.016954999417066574 0.9602699875831604 -0.071322001516819 -0.35558998584747314 -0.45969000458717346 -0.579200029373169 0.06953699886798859 0.0550290010869503 0.2514599859714508 -1.1691999435424805 -0.3417600095272064 0.7171099781990051 -0.26747000217437744 0.5116400122642517 0.47450000047683716 0.16333000361919403 0.7372400164604187 0.2549700140953064 -0.025102000683546066 -0.4036499857902527 -0.2348800003528595 0.7593100070953369 -0.7135599851608276 -0.5720999836921692 -0.3852599859237671 0.2422800064086914 -0.011851999908685684 0.773829996585846 0.23239000141620636 -0.41391000151634216 -0.2872599959373474 0.26923999190330505 0.19836999475955963 1.0341999530792236 -0.6041200160980225 -0.36904001235961914 0.10730999708175659 0.38297998905181885 0.9104599952697754 T',\n",
       " 'involving VBG involving the DT the Sarcoidosis NN Sarcoidosis i in inv invo g ng ing ving n_D n_J n_A n_C n_B n_N n_E 1.0029000043869019 0.06729300320148468 -0.4431900084018707 1.0410000085830688 -0.2920199930667877 1.6878000497817993 0.2180899977684021 -0.19418999552726746 0.07922100275754929 -0.04172699898481369 -0.6098799705505371 -0.13346999883651733 -0.6302099823951721 0.16469000279903412 0.9158300161361694 -0.9635499715805054 -0.14282000064849854 0.42691999673843384 -0.5697799921035767 -0.37455999851226807 0.7154300212860107 0.18685999512672424 -0.002523500006645918 -0.0360339991748333 -0.731410026550293 -1.3183000087738037 0.4130899906158447 -0.5719599723815918 0.021080000326037407 0.1724800020456314 2.543100118637085 -0.2179500013589859 -0.11619000136852264 -0.800790011882782 0.30129000544548035 0.5988100171089172 -0.518060028553009 -0.5055800080299377 -0.40849000215530396 0.19577999413013458 -0.5190899968147278 0.23510999977588654 0.6111900210380554 0.8260300159454346 0.47328999638557434 -0.49480000138282776 -0.09257899969816208 0.9047200083732605 -0.10815999656915665 -0.321370005607605 O',\n",
       " 'the DT the veriform NN veriform involving VBG involving t th the the e he the the n_D y_J y_A y_C n_B y_N n_E 0.4180000126361847 0.24967999756336212 -0.41242000460624695 0.1216999962925911 0.3452700078487396 -0.044456999748945236 -0.4968799948692322 -0.17861999571323395 -0.0006602299981750548 -0.6565999984741211 0.2784300148487091 -0.14767000079154968 -0.5567700266838074 0.14657999575138092 -0.009509500116109848 0.011657999828457832 0.10204000025987625 -0.127920001745224 -0.8442999720573425 -0.12180999666452408 -0.016800999641418457 -0.33278998732566833 -0.15520000457763672 -0.23130999505519867 -0.1918099969625473 -1.8823000192642212 -0.7674599885940552 0.09905099868774414 -0.42124998569488525 -0.19526000320911407 4.0071001052856445 -0.1859399974346161 -0.5228700041770935 -0.3168100118637085 0.0005921300034970045 0.007444899994879961 0.17778000235557556 -0.15896999835968018 0.012040999718010426 -0.05422300100326538 -0.2987099885940552 -0.15749000012874603 -0.3475799858570099 -0.04563700035214424 -0.4425100088119507 0.1878499984741211 0.002784899901598692 -0.18411000072956085 -0.11513999849557877 -0.7858099937438965 O',\n",
       " 'veriform NN veriform appendix NN appendix the DT the v ve ver veri m rm orm form n_D n_J n_A n_C n_B n_N n_E 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 O',\n",
       " 'appendix NN appendix __END1__ __END1__ __END1__ veriform NN veriform a ap app appe x ix dix ndix n_D n_J y_A n_C n_B n_N n_E 0.8911899924278259 0.09548000246286392 -0.47795000672340393 -0.29148000478744507 -0.4202899932861328 0.9696699976921082 0.540880024433136 -0.984499990940094 1.5807000398635864 -0.6932500004768372 -0.08605500310659409 1.3145999908447266 -0.30935999751091003 0.4416399896144867 0.7297700047492981 -0.3591200113296509 -1.3178999423980713 -0.6029000282287598 -0.16719000041484833 0.9431099891662598 -1.1197999715805054 -0.9848700165748596 0.8771499991416931 -0.8819199800491333 0.054568998515605927 -0.23875999450683594 -0.2215700000524521 0.8864200115203857 0.3353799879550934 0.12621000409126282 0.7082200050354004 -0.7063999772071838 0.22991999983787537 0.4431599974632263 -0.8050500154495239 0.9938099980354309 1.7692999839782715 -0.14801999926567078 -0.4882499873638153 0.2251800000667572 0.2076999992132187 -0.74058997631073 0.001534900045953691 0.5338699817657471 -0.3465299904346466 1.1490999460220337 0.01081900019198656 0.028742000460624695 -0.05806000158190727 -0.17416000366210938 O']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0].get_in_format(True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2924/2924 [00:02<00:00, 1140.30it/s]\n"
     ]
    }
   ],
   "source": [
    "write_file(new_data,\"../input_data/temp.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 731/731 [00:00<00:00, 1161.05it/s]\n"
     ]
    }
   ],
   "source": [
    "write_file(new_test_data,\"../test_data/temp.txt\",label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
